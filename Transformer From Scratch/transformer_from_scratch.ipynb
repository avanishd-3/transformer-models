{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "tli0VrPj10G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "9uogWz2v1Lod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "mVUoFdIO1PaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_attention(query, key, value, mask=None):\n",
        "\n",
        "  key_dim = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(key_dim) # Matrix multiplication\n",
        "\n",
        "  if mask is not None: # Hide inputs for decoder\n",
        "    scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "  attention = F.softmax(scores, dim=-1)\n",
        "\n",
        "  return torch.bmm(attention, value)"
      ],
      "metadata": {
        "id": "ZFEhFxcF1Of1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, head_dim, mask=None):\n",
        "    # Head dim in practice is multiple of embed_dim to keep constant computation across each head\n",
        "    # Ex: BERT has 12 heads each w/ dimension 768/12 = 64\n",
        "\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.query = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "    self.key = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "    self.value = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "\n",
        "    self.mask = mask\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    return single_attention(self.query(hidden_state), self.key(hidden_state), self.value(hidden_state), self.mask)"
      ],
      "metadata": {
        "id": "4S1lb_5a2TsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Concatenate outputs of single attention heads to produce final output\n",
        "\n",
        "  Config is based on transformers.AutoConfig\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config, mask=None):\n",
        "    super().__init__()\n",
        "\n",
        "    embed_dim = config.hidden_size\n",
        "    num_heads = config.num_attention_heads\n",
        "    head_dim = embed_dim // num_heads\n",
        "\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(embed_dim, head_dim, mask) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        "    x = self.output_linear(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "83cVTnLt285N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed-forward"
      ],
      "metadata": {
        "id": "Dwzr537u3raL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "\n",
        "    self.gelu = nn.GELU() # Theorized to be best activation function for transformers\n",
        "\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear_1(x)\n",
        "\n",
        "    x = self.gelu(x)\n",
        "\n",
        "    x = self.linear_2(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "jokGCJo-3tRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization\n",
        "\n",
        "Layer normalization: Normalize each input in the batch to have 0 mean and same variance\n",
        "\n",
        "Skip connections: Pass a tensor to the next layer of model w/o processing"
      ],
      "metadata": {
        "id": "Q4BnCcfh5Ao5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pre-layer normalization\n",
        "\n",
        "# Layer Norm --> Multi-head Attention --> Layer Norm --> Feed forward\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "    self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "\n",
        "    self.attention = MultiHeadAttention(config)\n",
        "    self.feed_forward = FeedForwardNetwork(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    hidden_state = self.layer_norm_1(x) # First layer normalization\n",
        "    x = x + self.attention(hidden_state) # Attention w/ skip connection\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x)) # Feed forward & next layer normaliation w/ skip connection\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "kK0VybsT5Cre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embeddings\n",
        "\n",
        "Incorporate positional information (attention is just weighted sum, so it can't do this by itself)"
      ],
      "metadata": {
        "id": "5zG9VlU26Qzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Augment token embeddings w/ position index (can be learned by attention layer & feed forward network)\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "\n",
        "    seq_length = input_ids.size(-1)\n",
        "    position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) # (1, seq_length)\n",
        "\n",
        "    token_embeddings = self.token_embeddings(input_ids) # (batch_size, seq_length, hidden_size)\n",
        "    position_embeddings = self.position_embeddings(position_ids) # (1, seq_length, hidden_size)\n",
        "\n",
        "    embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "    embeddings = self.layer_norm(embeddings)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "JIH_1ZSC6SQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embeddings = PositionalEmbedding(config)\n",
        "    self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embeddings(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x # Get a hidden state for each token in the batch"
      ],
      "metadata": {
        "id": "MhYZH9rQ7ll0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification"
      ],
      "metadata": {
        "id": "loZVTW9e8Owd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceClassification(nn.Module):\n",
        "  \"\"\"\n",
        "  Need to define config.num_labels before using\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(config)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)[:, 0, :] # Select hidden state of tokens w/ []\n",
        "    x = self.dropout(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "FHeqd5za8QSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "ETbFiALD1LVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention\n",
        "\n",
        "Need to mask inputs to make sure that decoder is actually learning and not just copying (why mask is in previous attention stuff)\n",
        "\n",
        "Also need encoder-decoder attention layer to learn how to relate tokens from 2 different sequences"
      ],
      "metadata": {
        "id": "6MMjjTkS1atK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example mask\n",
        "\n",
        "ex_seq_len = inputs.input_ids.size(-1)\n",
        "ex_mask = torch.tril(torch.ones(ex_seq_len, ex_seq_len)).unsqueeze(0) # Creates lower triangular matrix"
      ],
      "metadata": {
        "id": "3MBqXLjz1MWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Encoder-decoder attention layer.\n",
        "  This allows the decoder to attend to encoder outputs.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.self_attention = MultiHeadAttention(config)\n",
        "\n",
        "  def forward(self, decoder_hidden_state, encoder_hidden_state):\n",
        "    return self.attention(decoder_hidden_state, encoder_hidden_state)"
      ],
      "metadata": {
        "id": "zcy59ax42zRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization"
      ],
      "metadata": {
        "id": "5aaMRFMV4VOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pre-layer normalization\n",
        "\n",
        "# Layer Norm --> Multi-head Attention --> Encoder-decoder attention --> Layer Norm --> Feed forward\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Decoder layer with masked self-attention, encoder-decoder attention, and feed-forward network.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config, mask):\n",
        "    self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "    self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "    self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n",
        "\n",
        "    self.attention = MultiHeadAttention(config, mask)\n",
        "    self.encoder_decoder_attention = EncoderDecoderAttention(config)\n",
        "\n",
        "    self.feed_forward = FeedForwardNetwork(config)\n",
        "\n",
        "  def forward(self, x, encoder_output):\n",
        "    hidden_state = self.layer_norm_1(x) # First layer normalization\n",
        "    x = x + self.attention(hidden_state) # Masked attention w/ skip connection\n",
        "\n",
        "\n",
        "    hidden_state = self.layer_norm_2(x) # Second layer normalization\n",
        "    x = x + self.encoder_decoder_attention(hidden_state, encoder_output) # Encoder-decoder attention\n",
        "\n",
        "    x = x + self.feed_forward(self.layer_norm_3(x)) # Feed forward & next layer normaliation w/ skip connection\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "eLuTrpPD4XZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embeddings"
      ],
      "metadata": {
        "id": "gtqdZd1f5OMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embeddings = PositionalEmbedding(config)\n",
        "\n",
        "    self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "  def forward(self, x, encoder_output):\n",
        "    x = self.embeddings(x)\n",
        "\n",
        "    # Generate mask dynamically based on sequence length\n",
        "    seq_length = x.size(1)  # Get sequence length\n",
        "    mask = self.create_mask(seq_length, x.device)  # Ensure mask is on the same device\n",
        "\n",
        "    for layer in self.layers:\n",
        "        x = layer(x, encoder_output, mask)  # Pass the mask to each decoder layer\n",
        "\n",
        "    return x  # Return hidden states\n",
        "\n",
        "  def create_mask(self, seq_length, device):\n",
        "    return torch.tril(torch.ones(seq_length, seq_length, device=device)).unsqueeze(0) # Create lower triangular matrix"
      ],
      "metadata": {
        "id": "auBPAivO5Qju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}